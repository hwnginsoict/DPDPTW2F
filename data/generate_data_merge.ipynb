{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\CodingEnvironment\\\\DPDPTW2F\\\\data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\CodingEnvironment\\DPDPTW2F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from graph.node import Node\n",
    "from graph.request import Request\n",
    "\n",
    "def create_requests(file_path, output_path):\n",
    "    # Read and parse the input file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    node_list = []\n",
    "    with open(file_path, 'rt') as f:\n",
    "        count = 1\n",
    "        for line in f:\n",
    "            # Read vehicle parameters from the first line\n",
    "            if count == 1:\n",
    "                vehicle_num, vehicle_cap, vehicle_speed = line.split()\n",
    "                # Adjust vehicle_num as per original logic\n",
    "                vehicle_num = int(int(vehicle_num) / 2.5)\n",
    "                vehicle_cap = int(vehicle_cap)\n",
    "                vehicle_speed = float(vehicle_speed)\n",
    "            else:\n",
    "                node_list.append(line.split())\n",
    "            count += 1\n",
    "\n",
    "    # Create Node instances from the parsed data\n",
    "    nodes = [\n",
    "        Node(\n",
    "            nid=int(item[0]),\n",
    "            x=float(item[1]),\n",
    "            y=float(item[2]),\n",
    "            demand=float(item[3]),\n",
    "            ready_time=float(item[4]),\n",
    "            due_time=float(item[5]),\n",
    "            service_time=float(item[6]),\n",
    "            pid=int(item[7]),\n",
    "            did=int(item[8]),\n",
    "            time=-1\n",
    "        ) for item in node_list\n",
    "    ]\n",
    "\n",
    "    def create_requests_and_save_to_csv(nodes, filename):\n",
    "        # Create a lookup dictionary for nodes by their nid for fast access\n",
    "        node_lookup = {node.nid: node for node in nodes}\n",
    "\n",
    "        requests = []\n",
    "        request_id = 1\n",
    "\n",
    "        # Process nodes to create merged requests\n",
    "        for node in nodes:\n",
    "            # Only consider nodes that reference another node via pid or did\n",
    "            if node.pid != 0 or node.did != 0:\n",
    "                # Determine pickup node: if pid exists, use the referenced node; otherwise, use current node\n",
    "                if node.pid != 0 and node.pid in node_lookup:\n",
    "                    pnode = node_lookup[node.pid]\n",
    "                else:\n",
    "                    pnode = node\n",
    "\n",
    "                # Determine delivery node: if did exists, use the referenced node; otherwise, use current node\n",
    "                if node.did != 0 and node.did in node_lookup:\n",
    "                    dnode = node_lookup[node.did]\n",
    "                else:\n",
    "                    dnode = node\n",
    "\n",
    "                # Create a merged request using data from the pickup and delivery nodes\n",
    "                request = Request(\n",
    "                    nid=request_id,\n",
    "                    px=pnode.x,\n",
    "                    py=pnode.y,\n",
    "                    dx=dnode.x,\n",
    "                    dy=dnode.y,\n",
    "                    pready=pnode.ready_time,\n",
    "                    pdue=pnode.due_time,\n",
    "                    dready=dnode.ready_time,\n",
    "                    ddue=dnode.due_time,\n",
    "                    demand=node.demand,\n",
    "                    service_time=node.service_time,\n",
    "                    time=node.time\n",
    "                )\n",
    "                requests.append(request)\n",
    "                request_id += 1\n",
    "\n",
    "        # Save the merged requests to a CSV file\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write header\n",
    "            writer.writerow([\n",
    "                \"nid\", \"px\", \"py\", \"dx\", \"dy\", \"pready\", \"pdue\", \n",
    "                \"dready\", \"ddue\", \"demand\", \"service_time\", \"time\"\n",
    "            ])\n",
    "            # Write each request's data\n",
    "            for req in requests:\n",
    "                writer.writerow([\n",
    "                    req.nid, req.px, req.py, req.dx, req.dy, \n",
    "                    req.pready, req.pdue, req.dready, req.ddue, \n",
    "                    req.demand, req.service_time, req.time\n",
    "                ])\n",
    "\n",
    "    # Generate requests and save them to CSV\n",
    "    create_requests_and_save_to_csv(nodes, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for type in [8]:\n",
    "    for i in range(1, 11):\n",
    "        for dis in ['LC1', 'LC2', 'LR1', 'LR2', 'LRC1', 'LRC2']:\n",
    "            file_path = f'data/original/pdp_{type}00/{dis}_{type}_{i}.txt'\n",
    "            output_path = f'data/merge/{type}00/{dis}_{type}_{i}.csv'\n",
    "            create_requests(file_path, output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive F is New Volume\n",
      " Volume Serial Number is EC26-6E9F\n",
      "\n",
      " Directory of f:\\CodingEnvironment\\DPDPTW2F\n",
      "\n",
      "01/02/2025  02:02 AM    <DIR>          .\n",
      "01/02/2025  02:02 AM    <DIR>          ..\n",
      "12/30/2024  05:18 PM    <DIR>          algorithm\n",
      "01/02/2025  01:43 AM    <DIR>          data\n",
      "01/02/2025  01:12 AM    <DIR>          graph\n",
      "01/02/2025  02:02 AM                 0 problem.py\n",
      "               1 File(s)              0 bytes\n",
      "               5 Dir(s)  62,406,746,112 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
